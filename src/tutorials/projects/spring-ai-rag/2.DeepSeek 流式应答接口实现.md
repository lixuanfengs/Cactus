---
title: "DeepSeek 流式应答接口实现"
subtitle: "2.DeepSeek 流式应答接口实现"
date: 2024-5-15 10:54:20
category:
  - Spring AI
tag:
  - Spring AI
order: 2
---

## 一、 技术架构

本章主要介绍。集成 Spring AI 框架的组件，通过对接 Ollama DeepSeek 实现服务接口的开发，包括普通应答接口和流式应答接口。

目前对接 AI 的方式多种多样，例如：通过 AI 官网提供的 SDK、基于 自研 SDK 组件 或采用 One-API 服务类统一封装接口。为了探索不同的技术方案并提升学习多样性，本项目将采用 Spring AI 框架来完成与 Ollama DeepSeek 的对接。

![image-20250314173549866](https://beauties.eu.org/blogimg/main/img1/image-20250314173549866.png)

官网：[spring-ai](https://spring.io/projects/spring-ai)

Spring AI 支持；OpenAI，Azure AI，Amazon，Google和Ollama，大模型的对接。其他不属于这个范围的，可以通过 [one-api](https://github.com/songquanpeng/one-api) 配置，统一转换为 OpenAI 接口服务格式进行使用。

## 二、功能实现

### 1. 项目结构

![image-20250314172135393](https://beauties.eu.org/blogimg/main/img1/image-20250314172135393.png)

### 2. 导入框架

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-bom</artifactId>
    <version>${spring-ai.version}</version>
    <type>pom</type>
    <scope>import</scope>
</dependency>
```

>  spring-ai 父pom，定义引入ai框架的适配版本。在统一父pom下，所有的spring-ai下的组件，都不需要再单独定义pom版本。
>
> 注意；这样的设计手段，你也可以在以后自己定义的一些组件中使用，不要让使用方各种适配。



```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-ollama</artifactId>
</dependency>
```

> 之后在具体使用ai调用方法的的工程模块下，引入对应的pom。如，本节我们要对接 ollama 就要引入 `spring-ai-ollama`。

### 3. 接口配置

**application-dev.yml**

```yml
server:
  port: 7080

spring:
  ai:
    ollama:
      base-url: http://192.168.1.107:11434


# Redis
redis:
  sdk:
    config:
      host: 192.168.1.107
      port: 16379
      pool-size: 10
      min-idle-size: 5
      idle-timeout: 30000
      connect-timeout: 5000
      retry-attempts: 3
      retry-interval: 1000
      ping-interval: 60000
      keep-alive: true

logging:
  level:
    root: info
  config: classpath:logback-spring.xml

```

>  换成你的 Ollama 所在的服务IP，云服务器就是公网IP，注意开放端口 11434

### 4. 核心代码

```java
package cn.cactusli.lxf.rag.trigger.http;

import cn.cactusli.lxf.rag.api.IAiService;
import jakarta.annotation.Resource;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.ai.chat.prompt.Prompt;
import org.springframework.ai.ollama.OllamaChatModel;
import org.springframework.ai.ollama.api.OllamaOptions;
import org.springframework.web.bind.annotation.*;
import reactor.core.publisher.Flux;

/**
 * Package: cn.cactusli.lxf.rag.trigger.http
 * Description:
 *
 * @Author 仙人球⁶ᴳ |
 * @Date 2025/3/14 14:42
 * @Github https://github.com/lixuanfengs
 */
@RestController
@CrossOrigin({"*"})
@RequestMapping("/api/v1/ollama/")
public class OllamaController implements IAiService {

    @Resource
    private OllamaChatModel chatModel;

    /**
     * http://localhost:7080/api/v1/ollama/generate?model=deepseek-r1:1.5b&message=你是？
     */
    @GetMapping("generate")
    @Override
    public ChatResponse generate(@RequestParam String model, @RequestParam String message) {
        return chatModel.call(new Prompt(message, OllamaOptions.builder().model(model).build()));
    }

    /**
     * http://localhost:7080/api/v1/ollama/generate_stream?model=deepseek-r1:1.5b&message=你是？
     */
    @GetMapping("generate_stream")
    @Override
    public Flux<ChatResponse> generateStream(@RequestParam String model, @RequestParam String message) {
        return chatModel.stream(new Prompt(message, OllamaOptions.builder().model(model).build()));
    }
}

```

> Spring AI 提供对话的接口非常简单，call 是直接应答，stream 是流式应答。流式应答通过 Flux 返回。
>
> Project Reactor 是一个用于构建响应式应用程序的库，Flux 是 Reactor 中的一个核心组件，用于表示一个异步序列，可以发出 0 到 N 个元素，并且可以是有限的或无限的流

## 三、验证测试

确保，Docker Ollama DeepSeek 运行正常。之后启动本地服务 SpringBoot 检查日志是否运行正常。

访问接口：http://localhost:7080/api/v1/ollama/generate_stream?model=deepseek-r1:1.5b&message=你是？

![image-20250314173312429](https://beauties.eu.org/blogimg/main/img1/image-20250314173312429.png)

> 如图，可以看到接口的反馈结果。这些结果可以用于后续提供给前端页面做流式展示。